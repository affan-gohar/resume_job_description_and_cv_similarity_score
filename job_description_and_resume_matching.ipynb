{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Affan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import docx2txt\n",
    "from pdfminer.high_level import extract_text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = input(\"Enter the paths to your resume files, separated by commas: \").split(',')\n",
    "filetype= input(\"Enter the file type of your resumes (word/pdf): \").strip().lower()\n",
    "\n",
    "#  job description\n",
    "jobdescription = input(\"Enter the job description text: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Texts from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from word doc\n",
    "def extract_text_from_word(file_path):\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "# extracting from pdf\n",
    "def extract_text_from_pdf(file_path):\n",
    "    return extract_text(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Skills, Education, Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting skills education years of experience\n",
    "def extract_resume_sections(text):\n",
    "    skills = extract_skills(text)\n",
    "    education = extract_education(text)\n",
    "    experience = extract_experience(text)\n",
    "    return skills, education, experience\n",
    "# skills\n",
    "# Function to extract skills\n",
    "def extract_skills(text):\n",
    "    skills_pattern = re.compile(r\"(skills|technical skills|programming languages)(.*?)(education|experience|work experience)\", re.IGNORECASE | re.DOTALL)\n",
    "    skills = skills_pattern.search(text)\n",
    "    return skills.group(2).strip() if skills else \"\"\n",
    "\n",
    "# education\n",
    "# Function to extract education\n",
    "def extract_education(text):\n",
    "    education_pattern = re.compile(r\"(education)(.*?)(experience|skills|work experience)\", re.IGNORECASE | re.DOTALL)\n",
    "    education = education_pattern.search(text)\n",
    "    return education.group(2).strip() if education else \"\"\n",
    "\n",
    "# experience\n",
    "# Function to extract years of experience\n",
    "def extract_experience(text):\n",
    "    experience_pattern = re.compile(r\"(experience|work experience)(.*)\", re.IGNORECASE | re.DOTALL)\n",
    "    experience = experience_pattern.search(text)\n",
    "    return experience.group(2).strip() if experience else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making tokens of input texts\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate average Word2Vec vector for a section of text\n",
    "def average_word2vec_vector(model, text):\n",
    "    tokens = tokenize(text)\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)    #making vectors of skills education and experince f founf if not found return zero(0)\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring cosine similarity accross different vectors\n",
    "def calculate_similarity_with_word2vec(model, resume_section, job_section):\n",
    "    resume_vector = average_word2vec_vector(model, resume_section)\n",
    "    job_vector = average_word2vec_vector(model, job_section)\n",
    "    return cosine_similarity([resume_vector], [job_vector])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfs\n",
    "resume_files = resume\n",
    "file_type = filetype\n",
    "\n",
    "#  job description\n",
    "job_description = jobdescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.pdf', 'affangohar_cv.pdf']\n",
      "pdf\n",
      "LN Technologies, a subsidiary of App Maisters Inc USA, is an Enterprise App Development company that specializes in developing Mobility, Web, BI and Cloud based solutions. With over 400 successful projects under our portfolio, we take pride in catering to the needs of all sizes and types of clients, from ambitious startups & small businesses to large multinationals.  We are currently seeking highly motivated Engineers in Karachi office to join our team. The ideal candidates should have a strong technical background, possess the ability to work independently as well as collaboratively, and have some experience in designing, developing, testing, and implementing engineering projects. The engineers will play a key role in building AI applications and will be given significant responsibility within a small startup environment.  Job Requirements:  Moderate experience in Python: Proficiency in Python programming language. Demonstrated experience in developing and maintaining Python applications. Familiarity with Python libraries and frameworks commonly used in AI development. Proficiency in programming basics such as loops, conditional logic, file and package management: Solid understanding of fundamental programming concepts and logic. Ability to implement loops, conditional statements, and effectively manage files and packages. Experience with version control systems like Git would be advantageous. Knowledge of interacting with APIs, specifically OpenAI's APIs for embeddings and ChatGPT models: Familiarity with RESTful API principles and experience in integrating and consuming APIs. Understanding of OpenAI's APIs, including effective interaction with embeddings and ChatGPT models. Ability to handle authentication, error handling, and data parsing while working with APIs. Ability to work with various data formats including JSON, CSV, Parquet, and TXT: Proficiency in handling and processing data in different formats, such as JSON, CSV, Parquet, and TXT. Knowledge of data manipulation techniques, data cleaning, and data transformation using Python. Experience with libraries like Pandas for efficient data handling and manipulation. Familiarity with Command Line Interfaces and web apps for code testing: Understanding of Command Line Interfaces (CLI) and experience in working with CLI tools for code testing, debugging, and deployment. Knowledge of web application frameworks like Flask or Django for developing and testing code. Ability to build and deploy code in a web app environment for efficient testing and validation. Benefits:  1) Provident Fund  2) Annual Leaves  3) Salary given on time (10th of every month)  4) Medical Insurance  5) Life insurance  6) Career Growth  7) Bi-Annual Bonus  Job Type: Full-time  Pay: Rs80,000.00 - Rs150,000.00 per month  Application Question(s):  What is your current gross salary? What is your expected gross salary? What is the reason of leaving current Job? In how much time u can join us, if shortlisted? How many years you have worked in AI/ML? How many years you have worked in Software House in Mobile Apps as AI/ML? Which python libraries have you been working on? Have you trained any ML/DL model Experience:  AI/ML: 2 years (Preferred)\n"
     ]
    }
   ],
   "source": [
    "print(resume_files)\n",
    "print(file_type)\n",
    "print(job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat texts of Job description and Input CV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Affan/nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Affan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     all_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m resume_text\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize and train Word2Vec model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_text\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39mtokenized_text, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Extract job description sections\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(text):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Affan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Affan/nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Affan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Affan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Combine all text from the resumes and job description to train Word2Vec model\n",
    "all_text = job_description\n",
    "\n",
    "resume_texts = []\n",
    "for resume_file in resume_files:\n",
    "    if file_type == 'word':\n",
    "        resume_text = extract_text_from_word(resume_file.strip())\n",
    "    elif file_type == 'pdf':\n",
    "        resume_text = extract_text_from_pdf(resume_file.strip())\n",
    "    else:\n",
    "        print(\"Unsupported file type!\")\n",
    "        exit()\n",
    "\n",
    "    resume_texts.append(resume_text)\n",
    "    all_text += \" \" + resume_text\n",
    "# Tokenize and train Word2Vec model\n",
    "tokenized_text = [tokenize(all_text)]\n",
    "model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extract job description sections\n",
    "job_skills, job_education, job_experience = extract_resume_sections(job_description)\n",
    "\n",
    "# Initialize a dictionary to store overall similarity scores for each resume\n",
    "resume_scores = {}\n",
    "\n",
    "# For each resume, calculate similarity for skills, education, and experience\n",
    "for i, resume_text in enumerate(resume_texts):\n",
    "    resume_skills, resume_education, resume_experience = extract_resume_sections(resume_text)\n",
    "\n",
    "    # Calculate similarity scores for skills, education, and experience\n",
    "    skills_similarity = calculate_similarity_with_word2vec(model, resume_skills, job_skills)\n",
    "    education_similarity = calculate_similarity_with_word2vec(model, resume_education, job_education)\n",
    "    experience_similarity = calculate_similarity_with_word2vec(model, resume_experience, job_experience)\n",
    "\n",
    "    # Combine the scores (you can weigh them differently if necessary)\n",
    "    overall_score = (skills_similarity + education_similarity + experience_similarity) / 3\n",
    "    resume_scores[resume_files[i].strip()] = overall_score\n",
    "\n",
    "    print(f\"Resume {i+1} - {resume_files[i].strip()} similarity scores:\")\n",
    "    print(f\"  Skills Similarity: {skills_similarity:.2f}\")\n",
    "    print(f\"  Education Similarity: {education_similarity:.2f}\")\n",
    "    print(f\"  Experience Similarity: {experience_similarity:.2f}\")\n",
    "    print(f\"  Overall Similarity: {overall_score:.2f}\\n\")\n",
    "\n",
    "# Suggest the best resume\n",
    "best_resume = max(resume_scores, key=resume_scores.get)\n",
    "print(f\"The best matching resume for the job description is: {best_resume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize and train Word2Vec model\n",
    "# tokenized_text = [tokenize(all_text)]\n",
    "# model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Extract job description sections\n",
    "# job_skills, job_education, job_experience = extract_resume_sections(job_description)\n",
    "\n",
    "# # Initialize a dictionary to store overall similarity scores for each resume\n",
    "# resume_scores = {}\n",
    "\n",
    "# # For each resume, calculate similarity for skills, education, and experience\n",
    "# for i, resume_text in enumerate(resume_texts):\n",
    "#     resume_skills, resume_education, resume_experience = extract_resume_sections(resume_text)\n",
    "\n",
    "#     # Calculate similarity scores for skills, education, and experience\n",
    "#     skills_similarity = calculate_similarity_with_word2vec(model, resume_skills, job_skills)\n",
    "#     education_similarity = calculate_similarity_with_word2vec(model, resume_education, job_education)\n",
    "#     experience_similarity = calculate_similarity_with_word2vec(model, resume_experience, job_experience)\n",
    "\n",
    "#     # Combine the scores (you can weigh them differently if necessary)\n",
    "#     overall_score = (skills_similarity + education_similarity + experience_similarity) / 3\n",
    "#     resume_scores[resume_files[i].strip()] = overall_score\n",
    "\n",
    "#     print(f\"Resume {i+1} - {resume_files[i].strip()} similarity scores:\")\n",
    "#     print(f\"  Skills Similarity: {skills_similarity:.2f}\")\n",
    "#     print(f\"  Education Similarity: {education_similarity:.2f}\")\n",
    "#     print(f\"  Experience Similarity: {experience_similarity:.2f}\")\n",
    "#     print(f\"  Overall Similarity: {overall_score:.2f}\\n\")\n",
    "\n",
    "# # Suggest the best resume\n",
    "# best_resume = max(resume_scores, key=resume_scores.get)\n",
    "# print(f\"The best matching resume for the job description is: {best_resume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Suggested Resume/CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best CV/Resume\n",
    "# best_resume = max(resume_scores, key=resume_scores.get)\n",
    "# print(f\"The best matching resume for the job description is: {best_resume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
